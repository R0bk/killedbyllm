import React, { useState } from 'react';
import { Search, Calendar, ExternalLink, Info } from 'lucide-react';

const benchmarkData = [
  {
    name: "ARC-AGI",
    description: "Abstract reasoning challenge consisting of visual pattern completion tasks. Each task presents a sequence of abstract visual patterns and requires selecting the correct completion. Created by François Chollet as part of a broader investigation into measuring intelligence, the benchmark tests abstract reasoning capabilities similar to Raven's Progressive Matrices but with programmatically generated patterns.",
    dateCreated: "2019-11",
    dateDefeated: "2024-12",
    modelDefeated: "O3",
    originalScore: "Human Baseline: ~80%",
    finalScore: "O3: 87.5%",
    category: "Reasoning",
    creators: "François Chollet",
    organization: "Google",
    links: {
      paper: "https://arxiv.org/abs/1911.01547",
      website: "https://arcs-benchmark.org"
    },
    cause: "Saturation"
  },
  {
    name: "ARC (AI2)",
    description: "AI2 Reasoning Challenge (ARC) is a collection of reasoning tasks designed to test abstract reasoning abilities. Created by researchers at AI2, it includes grade-school level multiple-choice reasoning tasks such as logical deduction, spatial reasoning, and temporal reasoning. Each task requires models to apply abstract reasoning skills to solve problems that involve multiple steps and abstract concepts.",
    dateCreated: "2018-3",
    dateDefeated: "2023-03",
    modelDefeated: "GPT-4",
    originalScore: "Unspecifed",
    finalScore: "GPT-4: 96.3%",
    category: "Reasoning",
    creators: "Clark et al.",
    organization: "AI2",
    links: {
      paper: "https://arxiv.org/abs/1803.05457",
      website: "https://leaderboard.allenai.org/arc/submissions/get-started"
    },
    cause: "Saturation"
  },
  {
    name: "MATH",
    description: "A dataset of 12K challenging competition mathematics problems from the AMC, AIME, and other math competitions. Problems range from pre-algebra to olympiad-level and require complex multi-step reasoning. Each problem has a detailed solution that tests mathematical reasoning capabilities. Developed to measure mathematical problem-solving abilities in a way that requires genuine mathematical understanding.",
    dateCreated: "2021-03",
    dateDefeated: "2024-09",
    modelDefeated: "O1",
    originalScore: "Average CS PhD: ~40%",
    finalScore: "O1: 94.8%",
    category: "Mathematics",
    creators: "Hendrycks et al.",
    organization: "UC Berkeley",
    links: {
      paper: "https://arxiv.org/abs/2103.03874",
      github: "https://github.com/hendrycks/math"
    },
    cause: "Saturation"
  },
  {
    name: "HumanEval",
    description: "A collection of 164 Python programming problems designed to test language models' coding abilities. Created by OpenAI researchers to evaluate code generation capabilities. Each problem includes a function signature, docstring, and unit tests. Models must generate complete, correct function implementations that pass all test cases. Test set includes various programming concepts and difficulty levels.",
    dateCreated: "2021-07",
    dateDefeated: "2024-05",
    modelDefeated: "GPT-4o",
    originalScore: "Unspecified",
    finalScore: "GPT-4o: 90.2%",
    category: "Coding",
    creators: "Chen et al.",
    organization: "OpenAI",
    links: {
      paper: "https://arxiv.org/abs/2107.03374",
      github: "https://github.com/openai/human-eval"
    },
    cause: "Saturation"
  },
  {
    name: "SWAG",
    description: "A dataset of 113K multiple choice questions about grounded situations and commonsense inference. Given a partial description of a situation, models must predict what might happen next from 4 choices. Created using Adversarial Filtering to reduce annotation artifacts and biases, with counterfactuals generated by language models. Tests models' ability to reason about everyday situations using both natural language inference and commonsense reasoning.",
    dateCreated: "2018-05",
    dateDefeated: "2018-10", 
    modelDefeated: "BERT",
    originalScore: "Human: 88%",
    finalScore: "BERT: 86%",
    category: "Common Sense",
    creators: "Zellers et al.",
    organization: "UW & AI2",
    links: {
      paper: "https://arxiv.org/abs/1808.05326",
      website: "https://rowanzellers.com/swag/"
    },
    cause: "Saturation"
  },
  {
    name: "HellaSwag",
    description: "A challenging common sense reasoning dataset focused on grounded situations. Created by researchers at the University of Washington and AI2, it presents multiple-choice questions about everyday scenarios. The dataset uses adversarial filtering to ensure high quality and tests models' ability to understand and reason about real-world situations and their likely outcomes.",
    dateCreated: "2019-05",
    dateDefeated: "2023-03",
    modelDefeated: "GPT-4",
    originalScore: "Human: 95.6%",
    finalScore: "GPT-4: 95.3%",
    category: "Common Sense",
    creators: "Zellers et al.",
    organization: "UW & AI2",
    links: {
      paper: "https://arxiv.org/abs/1905.07830",
      website: "https://rowanzellers.com/hellaswag/"
    },
    cause: "Saturation"
  },
  {
    name: "MMLU",
    description: "Massive Multitask Language Understanding benchmark covering 57 subjects including mathematics, history, law, computer science, and more. Created by researchers at UC Berkeley, questions are drawn from real-world sources like professional exams. Tests both breadth and depth of knowledge across diverse academic and professional domains.",
    dateCreated: "2020-09",
    dateDefeated: "2023-03",
    modelDefeated: "GPT-4",
    originalScore: "95th pct Human: 87.0%",
    finalScore: "GPT-4: 87.3%",
    category: "Knowledge",
    creators: "Hendrycks et al.",
    organization: "UC Berkeley",
    links: {
      paper: "https://arxiv.org/abs/2009.03300",
      github: "https://github.com/hendrycks/test"
    },
    cause: "Saturation"
  },
  {
    name: "GSM8K",
    description: "Grade School Math 8K dataset contains 8.5K high-quality grade school math word problems. Developed by OpenAI researchers, each problem is broken down into steps, testing models' ability to perform multi-step mathematical reasoning. Problems require both numerical computation and natural language understanding.",
    dateCreated: "2021-10",
    dateDefeated: "2023-11",
    modelDefeated: "GPT-4",
    originalScore: "Unspecified",
    finalScore: "GPT-4: 92.0%",
    category: "Mathematics",
    creators: "Cobbe et al.",
    organization: "OpenAI",
    links: {
      paper: "https://arxiv.org/abs/2110.14168",
      github: "https://github.com/openai/grade-school-math"
    },
    cause: "Saturation",
    causeDetails: "GSM8K is often considered contaminated because of its inclusion in several instruction following datasets"
  },
  {
    name: "WSC",
    description: "Winograd Schema Challenge (WSC) is a benchmark consisting of carefully crafted sentence pairs that differ by one or two words and contain pronouns with ambiguous references that resolve differently based on those changes. The challenge was designed to be easily solved by humans but resistant to statistical methods and simple linguistic rules.",
    dateCreated: "2012-05",
    dateDefeated: "2019-07", 
    modelDefeated: "ROBERTA (w SFT)",
    originalScore: "Human: 96.5%",
    finalScore: "ROBERTA (w SFT): 90.1%",
    category: "Common Sense",
    creators: "Levesque et al.",
    organization: "U of T & NYU",
    links: {
      paper: "https://cdn.aaai.org/ocs/4492/4492-21843-1-PB.pdf",
      website: "https://cs.nyu.edu/~davise/papers/WinogradSchemas/WS.html"
    },
    cause: "Saturation"
  },
  {
    name: "WinoGrande",
    description: "An enhanced version of the Winograd Schema Challenge with 44K problems, designed to be robust against spurious statistical patterns. Created by researchers at AI2, it tests common-sense reasoning through pronoun resolution tasks that require real-world understanding. Created using adversarial filtering to ensure high quality.",
    dateCreated: "2019-07",
    dateDefeated: "2023-03",
    modelDefeated: "GPT-4",
    originalScore: "Human: 94%",
    finalScore: "GPT-4: 87.5%",
    category: "Common Sense",
    creators: "Sakaguchi et al.",
    organization: "AI2 & UW",
    links: {
      paper: "https://arxiv.org/abs/1907.10641",
      website: "https://winogrande.allenai.org/"
    },
    cause: "Saturation",
  },
  // {
  //   name: "TruthfulQA",
  //   description: "Tests models' ability to identify and avoid reproducing common misconceptions and falsehoods. Created by researchers at Anthropic and Oxford, questions cover topics where humans are known to hold false beliefs. Evaluates both truthfulness and informativeness of model responses. Includes adversarially-designed questions targeting known model biases.",
  //   dateCreated: "2021-09",
  //   dateDefeated: "2023-07",
  //   modelDefeated: "",
  //   originalScore: "Human: 94.8%",
  //   finalScore: "",
  //   category: "Truth",
  //   creators: "Lin, Hilton, Evans",
  //   organization: "OpenAI & Oxford",
  //   links: {
  //     paper: "https://arxiv.org/abs/2109.07958",
  //     github: "https://github.com/sylinrl/TruthfulQA"
  //   },
  //   cause: "Saturation & Contamination (Inclusion in several instruction following datasets"
  // },
  {
    name: "BIG-Bench",
    description: "Beyond the Imitation Game benchmark - a collaborative collection of 204 tasks spanning linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development and more. Led by Google researchers with contributions from the broader AI community, tasks are designed to probe specific capabilities and limitations of language models.",
    dateCreated: "2021-06",
    dateDefeated: "2022-04",
    modelDefeated: "Palm 540B",
    originalScore: "Human: 49.8%",
    finalScore: "Palm 540B: 61.4%",
    category: "Multi-task",
    creators: "Srivastava et al.",
    organization: "Google et al.",
    links: {
      paper: "https://arxiv.org/abs/2206.04615",
      github: "https://github.com/google/BIG-bench"
    },
    cause: "Saturation",
    causeDetails: "BIG-Bench further faces contamination challenges as: (a) Its canary string has been reproduced by many major models (b) Its contamination has been highlighted in many papers i.e. GPT-4 technical report"
  },
  {
    name: "BIG-Bench-Hard",
    description: "A suite of 23 challenging tasks from BIG-Bench where language models initially performed below average human-rater level.",
    dateCreated: "2022-10",
    dateDefeated: "2024-06", 
    modelDefeated: "Sonnet 3.5",
    originalScore: "Average Human: 67.7%",
    finalScore: "Sonnet 3.5: 93.1%",
    category: "Multi-task",
    creators: "Suzgun et al.",
    organization: "Google & Stanford",
    links: {
      paper: "https://arxiv.org/abs/2210.09261",
      github: "https://github.com/suzgunmirac/BIG-Bench-Hard"
    },
    cause: "Saturation"
  },
  {
    name: "SuperGLUE",
    description: "Successor to GLUE benchmark, featuring more challenging language understanding tasks. Created by researchers at NYU, Facebook AI, and other institutions, it includes word sense disambiguation, causal reasoning, and reading comprehension. Designed to be more difficult than GLUE while maintaining similar evaluation principles.",
    dateCreated: "2019-05",
    dateDefeated: "2019-10",
    modelDefeated: "T5",
    originalScore: "Human: 89.8%",
    finalScore: "T5: 89.3%",
    category: "Language",
    creators: "Wang et al.",
    organization: "NYU & Facebook AI",
    links: {
      paper: "https://arxiv.org/abs/1905.00537",
      website: "https://super.gluebenchmark.com/"
    },
    cause: "Saturation"
  },
  {
    name: "GLUE",
    description: "General Language Understanding Evaluation (GLUE) benchmark is a collection of nine tasks for evaluating natural language understanding systems. Created by researchers at NYU, UW, and DeepMind, it includes single-sentence tasks, similarity and paraphrase tasks, and inference tasks. Served as the primary NLU benchmark before SuperGLUE.",
    dateCreated: "2018-05",
    dateDefeated: "2020-07",
    modelDefeated: "XLNet",
    originalScore: "Human: 87.1%",
    finalScore: "XLNet: 88.4%",
    category: "Language",
    creators: "Wang et al.",
    organization: "NYU, UW & DeepMind",
    links: {
      paper: "https://arxiv.org/abs/1804.07461",
      website: "https://gluebenchmark.com/"
    },
    cause: "Saturation"
  },
  {
    name: "IFEval",
    description: "A comprehensive emergent ability evaluation suite consisting of coding, math, roleplay, and other tasks. Created to test instruction following capabilities of language models across diverse scenarios. Tests include complex multi-step instructions, constraint satisfaction, and error handling.",
    dateCreated: "2023-11",
    dateDefeated: "2024-03",
    modelDefeated: "LLama 3.3 70B",
    originalScore: "Unspecified",
    finalScore: "LLama 3.3 70B: 92.1%",
    category: "Instruction Following",
    creators: "Askell et al.",
    organization: "Google & Yale",
    links: {
      paper: "https://arxiv.org/abs/2311.07911",
      github: "https://github.com/google-research/google-research/tree/master/instruction_following_eval"
    },
    cause: "Saturation"
  },
  {
    name: "SQuAD",
    description: "Stanford Question Answering Dataset is a reading comprehension dataset consisting of questions posed by crowdworkers on Wikipedia articles. The answers to the questions are text segments from the corresponding reading passage. Version 1.1 contains 100,000+ question-answer pairs on 500+ articles. Note: We measure against v1.1.",
    dateCreated: "2016-05",
    dateDefeated: "2019-03",
    modelDefeated: "BERT",
    originalScore: "Human: 91.2%",
    finalScore: "BERT: 93.2%",
    category: "Language",
    creators: "Rajpurkar et al.",
    organization: "Stanford",
    links: {
      paper: "https://arxiv.org/abs/1606.05250",
      website: "https://rajpurkar.github.io/SQuAD-explorer/"
    },
    cause: "Saturation"
  },
  {
    name: "SQuAD v2.0",
    description: "An extension of SQuAD that adds unanswerable questions to the dataset. Models must not only answer questions when possible but also determine when no answer is supported by the passage. This tests both reading comprehension and the ability to identify when information is insufficient to answer a query.",
    dateCreated: "2018-05",
    dateDefeated: "2019-04",
    modelDefeated: "BERT",
    originalScore: "Human: 89.5%",
    finalScore: "BERT: 89.5%",
    category: "Language",
    creators: "Rajpurkar et al.",
    organization: "Stanford",
    links: {
      paper: "https://arxiv.org/abs/1806.03822",
      website: "https://rajpurkar.github.io/SQuAD-explorer/"
    },
    cause: "Saturation"
  },
  {
    name: "TriviaQA",
    description: "A large-scale reading comprehension dataset containing over 650K question-answer-evidence triples. Questions are authored by trivia enthusiasts and independently gathered evidence documents from Wikipedia and the web. The dataset is notable for requiring cross-sentence reasoning and synthesis of information from multiple sources.",
    dateCreated: "2017-05",
    dateDefeated: "2019-06",
    modelDefeated: "SpanBERT",
    originalScore: "Human: 79.7%",
    finalScore: "SpanBERT: 83.6%",
    category: "Knowledge",
    creators: "Joshi et al.",
    organization: "UW & AI2",
    links: {
      paper: "https://arxiv.org/abs/1705.03551",
      website: "http://nlp.cs.washington.edu/triviaqa/"
    },
    cause: "Saturation"
  }
];


// # TODO
// ## Must add:
// Turing Test!!!

// ## Already dead
// MGSM - Claude 3 Opus 90.7
// DROP - Sonnet 3.5 88.3
// DocVQA - Sonnet 3.5 95.2
// DocVQA - LLama 3.2 90.1
// DocVQA - Gemmini 1.0 Ultra 90.9
// ChartQA - Sonnet 3.5 90.8
// AI2D - Sonnet 3.5 95.3
// AI2D - Llama 3.2 92.3
// AIME 24 - O3 96.7

// ## Soon to die
// GPQA - O3 87.7
// MMLU-pro

// ## Needs confirmation
// MMMU


// ## Meaning of Saturation for Killed by LLM:
// That a benchmark can no longer measure the frontier. Not to say these are not still increadibly useful benchmarks - they are - however they are no longer relevent to the question of "can AI do X?"

// ## Difficulities in collecting data
// this is a best effort - please contribute if there are any mistakes
// to illulstrate the difficulitities
// 1. See these results here for qwen 2.5 72B instruct
// img 1
// img 2
// img 3
// 
// 2. or who should be credited on SQuAD 1.1?
// bert did not beat the human score in their original paper but in the SpanBERT paper they reimplemented BERT themselve and were able to beat the human score with the BERT architecture

const BenchmarkCard = ({ benchmark }) => {
  // Calculate lifespan
  const created = new Date(benchmark.dateCreated);
  const defeated = new Date(benchmark.dateDefeated);
  const diffMonths = Math.floor((defeated - created) / (1000 * 60 * 60 * 24 * 30));
  const years = Math.floor(diffMonths / 12);
  const months = diffMonths % 12;
  const lifespan = years 
    ? months 
      ? `Lived ${years} years ${months} months`
      : `Lived ${years} years`
    : `Lived ${months} months`;

  // Get years for timeline
  const startYear = created.getFullYear();
  const endYear = defeated.getFullYear();

  return (
    <div className="group relative benchmark-card">
      <div className="absolute inset-0 bg-gradient-to-r from-gray-900/50 via-red-900/20 to-gray-900/50 rounded-lg opacity-0 group-hover:opacity-100 transition-opacity duration-500" />
      
      <div className="relative bg-gray-950 rounded-lg p-4 border border-gray-800/50 backdrop-blur-xl transition-all duration-500 group-hover:border-gray-700/50">
        {/* Header */}
        <div className="flex items-start justify-between gap-4 mb-3">
          {/* Left side */}
          <div className="flex-1">
            <h3 className="text-lg font-medium tracking-tight text-white benchmark-name mb-1">
              {benchmark.name}
              {/* <div className="flex items-center text-gray-400 z-10 bg-gray-950 pr-2"> */}
                <span className="text-gray-400 bg-gray-950 text-xs pl-1"> ({startYear} - {endYear})</span>
              {/* </div> */}
            </h3>
            <div className="flex items-center gap-2">
              <span className="px-2 py-0.5 text-[10px] font-medium text-gray-400 bg-gray-900 rounded tracking-wide benchmark-category">
                {benchmark.category}
              </span>
              {/* <span className="text-xs text-red-400">
                {lifespan}
              </span> */}
            </div>
          </div>
          
          {/* Right side - Cause of death with tooltip */}
          <div className="flex flex-col items-end">
            <span className="text-[10px] uppercase tracking-wider text-gray-500">
              Killed by
            </span>
            <div className="flex items-center gap-1">
              <span className="text-sm font-medium text-red-400">
                {benchmark.cause}
              </span>
              {benchmark.causeDetails && (
                <div className="group/tooltip relative">
                  <Info size={12} className="text-gray-500 cursor-help" />
                  <div className="absolute bottom-full right-0 mb-2 w-64 p-2 bg-gray-900 rounded-lg border border-gray-700 
                                text-xs text-gray-300 opacity-0 invisible group-hover/tooltip:opacity-100 
                                group-hover/tooltip:visible transition-all duration-200 z-20">
                    {benchmark.causeDetails}
                  </div>
                </div>
              )}
            </div>
            {benchmark.links?.paper && (
              <a href={benchmark.links.paper} 
                 target="_blank" 
                 rel="noopener noreferrer"
                 className="text-gray-500 hover:text-white transition-colors mt-1">
                <ExternalLink size={14} />
              </a>
            )}
          </div>
        </div>

        {/* Description */}
        <p className="text-sm leading-relaxed text-gray-300 mb-3 line-clamp-2 group-hover:line-clamp-none benchmark-description">
          {benchmark.description}
        </p>

        {/* Simplified dates with dotted line */}
        <div className="relative flex justify-between text-xs mb-3">
          {/* <div className="flex items-center text-gray-400 z-10 bg-gray-950 pr-2">
            <span>{startYear}</span>
          </div> */}
          {/* Dotted line with gradient */}
          {/* <div className="absolute top-1/2 left-[15%] right-[15%] h-px" 
               style={{ 
                 backgroundImage: `repeating-linear-gradient(to right, 
                   transparent, 
                   transparent 4px,
                   rgb(31,41,55) 4px,
                   rgb(31,41,55) 8px
                 ), linear-gradient(to right, 
                   rgb(31,41,55), 
                   rgb(160,29,29)
                 )`,
                 backgroundSize: '8px 1px, 100% 1px',
               }} />
          <div className="flex items-center text-red-400 z-10 bg-gray-950 pl-2">
            <span>{endYear}</span>
          </div> */}
        </div>

        {/* Scores */}
        <div className="border-t border-gray-800/50 pt-3">
          <div className="text-xs text-gray-400 mb-2">
            <span className="text-gray-300">Defeated by:</span> {benchmark.modelDefeated}
          </div>
          <div className="grid grid-cols-2 gap-2">
            <div className="rounded bg-gray-900 px-2 py-1.5">
              <div className="text-[10px] text-gray-500">Original Score</div>
              <div className="text-xs text-gray-200">{benchmark.originalScore}</div>
            </div>
            <div className="rounded bg-gray-900 px-2 py-1.5">
              <div className="text-[10px] text-gray-500">Final Score</div>
              <div className="text-xs text-red-400">{benchmark.finalScore}</div>
            </div>
          </div>
        </div>
      </div>
    </div>
  );
};

const KilledByLLM = () => {
  const [searchTerm, setSearchTerm] = useState('');
  const [selectedCategory, setSelectedCategory] = useState('All');
  
  const categories = ['All', ...new Set(benchmarkData.map(b => b.category))];
  
  const filteredBenchmarks = benchmarkData
    .filter(benchmark => {
      const matchesSearch = 
        benchmark.name.toLowerCase().includes(searchTerm.toLowerCase()) ||
        benchmark.description.toLowerCase().includes(searchTerm.toLowerCase()) ||
        benchmark.creators.toLowerCase().includes(searchTerm.toLowerCase()) ||
        benchmark.organization.toLowerCase().includes(searchTerm.toLowerCase());
      const matchesCategory = selectedCategory === 'All' || benchmark.category === selectedCategory;
      return matchesSearch && matchesCategory;
    })
    .sort((a, b) => new Date(b.dateDefeated) - new Date(a.dateDefeated));
  
  return (
    <div className="min-h-screen bg-gradient-to-b from-black via-gray-950 to-black text-gray-100">
      <div className="absolute inset-0 bg-[radial-gradient(ellipse_at_top,_var(--tw-gradient-stops))] from-gray-900 via-gray-950 to-black" />
      
      {/* GitHub Link */}
      <div className="absolute top-4 right-4 z-10">
        <a
          href="https://github.com/R0bk/killedbyllm"
          target="_blank"
          rel="noopener noreferrer"
          className="flex items-center gap-2 text-gray-400 hover:text-white transition-colors"
        >
          <span className="text-sm">@R0bk/killedbyllm</span>
          <ExternalLink size={14} />
        </a>
      </div>
      
      <div className="relative max-w-7xl mx-auto px-6 py-12">
        <header className="text-center mb-16">
          <h1 className="text-5xl font-medium tracking-tight mb-4 bg-gradient-to-r from-white via-gray-100 to-gray-300 text-transparent bg-clip-text">
            Killed by LLM
          </h1>
          <p className="text-lg text-gray-400 tracking-wide">
            A memorial to the benchmarks that defined—and were defeated by—AI progress
          </p>
        </header>

        <div className="flex flex-col sm:flex-row gap-4 mb-12">
          <div className="relative flex-1">
            <Search className="absolute left-4 top-3.5 text-gray-500 pointer-events-none" size={18} />
            <input
              type="text"
              placeholder="Search benchmarks, creators, or organizations..."
              className="w-full pl-11 pr-4 py-3 bg-gray-900/50 backdrop-blur-sm border border-gray-800/50 rounded-xl 
                         focus:outline-none focus:border-gray-700 focus:ring-1 focus:ring-gray-700 
                         text-gray-200 placeholder-gray-500 transition-all"
              value={searchTerm}
              onChange={(e) => setSearchTerm(e.target.value)}
            />
          </div>
          
          <select
            className="px-4 py-3 bg-gray-900/50 backdrop-blur-sm border border-gray-800/50 rounded-xl
                       focus:outline-none focus:border-gray-700 focus:ring-1 focus:ring-gray-700
                       text-gray-200 appearance-none cursor-pointer min-w-[160px] transition-all"
            value={selectedCategory}
            onChange={(e) => setSelectedCategory(e.target.value)}
          >
            {categories.map(category => (
              <option key={category} value={category}>{category}</option>
            ))}
          </select>
        </div>

        <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-6">
          {filteredBenchmarks.map((benchmark, index) => (
            <BenchmarkCard key={index} benchmark={benchmark} />
          ))}
        </div>

        {/* Information Section */}
        <div className="mt-20 border-t border-gray-800/50 pt-16">
          <div className="grid grid-cols-1 md:grid-cols-2 gap-12">
            <div>
              <h2 className="text-2xl font-medium mb-4 bg-gradient-to-r from-white via-gray-100 to-gray-300 text-transparent bg-clip-text">
                Understanding "Saturation"
              </h2>
              <p className="text-gray-300 leading-relaxed">
                A benchmark is considered "saturated" when it can no longer effectively measure the frontier of AI capabilities. 
                While these benchmarks remain valuable tools for evaluation, they no longer serve as meaningful indicators for 
                answering the fundamental question: "Can AI perform this task?"
              </p>
            </div>

            <div>
              <h2 className="text-2xl font-medium mb-4 bg-gradient-to-r from-white via-gray-100 to-gray-300 text-transparent bg-clip-text">
                Data Collection Challenges
              </h2>
              <div className="text-gray-300 leading-relaxed space-y-4">
                <p>
                  This project represents a best-effort attempt to document AI's progress in defeating benchmarks. 
                  Attribution, timing and scores can be complex to determine definitively.
                </p>
                <p>
                  For example, if we examine Qwen 2.5 72B instruct's performance on MATH:
                  <ul>
                    <li>Qwen's technical report - 83.1</li>
                    <li>Stanford's HELM - 79.0</li>
                    <li>Huggingface's Open LLM Leaderboard - 38.7</li>
                  </ul>
                  These significant deviations make precise documentation challenging - hence please raise an issue or PR if you identify any discrepancies.
                </p>
                <p className="text-gray-400 text-sm">
                  Found an error or have additional data? Please contribute via GitHub.
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  );
};

export { KilledByLLM };